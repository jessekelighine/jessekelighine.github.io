\documentclass[a4paper,12pt]{article}
\usepackage{fontspec}\defaultfontfeatures{Ligatures=TeX}
\usepackage[a4paper,vmargin={4cm,4cm},hmargin={4cm,4cm}]{geometry}
% \usepackage{setspace}\setstretch{1.3} % \begin{spacing}{1.3}
%-----------------------------------------------------------------------------%
\usepackage{settings}
%-----------------------------------------------------------------------------%
%%% Title %%%
\title{Mallows's $C_p$}
\author{\texttt{jessekelighine.com}}
\date{\today}
%-----------------------------------------------------------------------------%

\begin{document}

\maketitle

\noindent
We are given a data set $\{x_{i},y_{i}\}_{i=1}^{n}$ where $x_{i}$'s are $k\times 1$ vectors and $y_{i}$'s are scalars.
It is known that the data generating process (DGP) is
\begin{gather*}
	y_{i} = \mu(x_{i}) + e_{i}
	\shortintertext{where}
	\mu(x_{i}) = x_{i}\T\beta
\end{gather*}
with $\E_{x_{i}}(e_{i})=0$ and $\var(e_{i}\given x_{i})=\sigma^{2}$.
\footnote{$\E_X(\cdot)$ denotes $\E(\cdot\given X)$.}
% \footnote{$\E_X(\cdot)$ denotes $\E(\cdot\given X)$; $I_n$ denotes a $n\times n$ identity matrix.}
Compactly, we can write the process as $Y=\mu(X)+E$ where
$Y=[y_{1},...,y_{n}]\T$,
$X=[x_{1},...,x_{n}]\T$, and
$E=[e_{1},...,e_{n}]\T$.

% with $\E_{X}(Y)=\mu$ and $\var(E\given X)=\sigma^{2}I_n$.
% That is, conditional on $X$, the expectation of $Y$ is fixed,
% and the variance of the error term is also fixed.

Our job is simple: to predict $y_{i}$ given $x_{i}$ using a linear model, i.e., to assess the ``fitness'' of the model.
However, how do we know which of $k$ the exogenous variables in $x_{i}$ should we choose to put in our model?
We want to find a way to measure how the good the prediction of a specific model would be.

The standard OLS estimator yields the estimator $\hat\beta=(X'X)\inv X'Y$.
An intuitive way of measuring prediction quality is to consider the expected sum of square errors:
\begin{align}\label{eq:in-sample}
	\E_{X} \left[ \sum_{i=1}^{n} ( y_{i} - x_{i}\T\hat\beta )^{2} \right]
	&= \E_{X} (Y-X\hat\beta)\T(Y-X\hat\beta) \nonumber\\
	% &= \frac1n\E_{X} [e\T(I-P)e] + \mu\T(I-P)\mu \\
	&= n\sigma^{2} - k\sigma^{2} + \mu\T(I_n-P)\mu \tag{in}
\end{align}
where $P=X(X\T X)\inv X\T$.
Notice the term $-k\sigma^{2}$.
This term suggests that the prediction error decreases as $k$, number of exogenous variables, increases.
That is, we can keep adding unrelated exogenous variables to the linear model and the prediction error will decrease!
Thus, this prediction error is not a good measure for how good the model is.

However, notice this this is only the case when we are doing ``in-sample'' prediction, i.e.,
evaluating prediction error with the data set that is used to produce $\hat\beta$.
We can consider calculating the prediction error using a hypothetical new data set with the same $x_{i}$'s but with different $y_{i}$'s,
denoted by $y_{i}^{\text{out}}$, generated according to the data generating process.
Using the new data set $\{x_{i},y_{i}^{\text{out}}\}$, we can compute the ``out-sample'' prediction error:
\begin{align}\label{eq:out-sample}
	\E_{X} \left[ \sum_{i=1}^{n} ( y_{i}^{\text{out}} - x_{i}\T\hat\beta )^{2} \right]
	&= \E_{X} (Y^{\text{out}}-X\hat\beta)\T(Y^{\text{out}}-X\hat\beta) \nonumber\\
	% &= \frac1n\E_{X} [e\T(I-P)e] + \mu\T(I-P)\mu \\
	&= n\sigma^{2} + k\sigma^{2} + \mu\T(I_n-P)\mu. \tag{out}
\end{align}
It is clear that the out-sample prediction error increases as $k$ increases.
Hence, out-sample prediction error is a much better criterion for evaluating the fitness of a model.

Now the practical question is: How can we calculate the out-sample prediction error when we only observe one data set?
The trick is to approximate the out-sample prediction error with the in-sample prediction error.
In fact, \eqref{eq:in-sample} and \eqref{eq:out-sample} are related by the simple equation
\begin{align}\label{eq:relation}
	\eqref{eq:out-sample} = \eqref{eq:in-sample} + 2k\sigma^2.
\end{align}
The term $2k\sigma^2$ can be viewed as an error correction term to $\eqref{eq:in-sample}$.
We can replace $\sigma^{2}$ by some estimator $\hat\sigma^{2}$ to obtain an estimate of \eqref{eq:out-sample}.
And that's basically it!

Formally, $C_{p}$ is defined as follows:
Suppose we have data $\{y_{i},x_{i}\}$ as before,
and we pick $p$ of the $k$ exogenous variables from $x_{i}$ to calculate the linear model coefficients $\hat\beta$, denoted by $\hat\beta_{p}$.
The $C_{p}$ for that choice of $p$ variables is defined by
\begin{align}\label{eq:cp}
	C_{p} \coloneqq 
	\frac1n\left( \sum_{i=1}^{n} (y_{i}-x_{i}\T\hat\beta_{p})^{2} + 2p\hat\sigma^{2} \right)
\end{align}
It is clear that \eqref{eq:cp} is simply \eqref{eq:relation} divided by $n$.
The $C_{p}$ values for different choices of $p$ tell us how the fitness of these models differ.
The choice of $p$ with the smallest $C_{p}$ is the most preferable.

\end{document}
