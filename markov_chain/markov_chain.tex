\documentclass[a4paper,12pt]{article}
\usepackage{fontspec}\defaultfontfeatures{Ligatures=TeX}
% \usepackage[a4paper,vmargin={3.5cm,3.5cm},hmargin={3.5cm,3.5cm}]{geometry}
\usepackage{settings}
%-----------------------------------------------------------------------------%
%%% Title %%%
\title{Markov Chain}
\author{\href{https://jessekelighine.com}{jessekelighine.com}}
\date{\today}
%-----------------------------------------------------------------------------%

\begin{document}

\maketitle

\begin{definition}[Markov Property]
	Let $S$ be a finite or countable set (state space).
	Let $\{X_{0},X_{1},...\}$ be a sequence of random variables whose ranges are in $S$.
	The sequence is said to satisfy the \emph{Markov property} if
	\begin{align*}
		\pr{X_{n+1}=j\given X_0=i_{0},...,X_n=i_{n}} =
		\pr{X_{n+1}=j\given X_n=i_{n}} =
		P_{ij}
	\end{align*}
	for all $i_{0},...,i_{n},j\in S$ and $n\in\naturals$.
	The probability $P_{ij}$ is called the transition probability from $i$ to $j$.
	The matrix $P$ whose $(i,j)$-entry equals $P_{ij}$ is called the \emph{transition matrix}.
\end{definition}

\begin{definition}[Markov Chain]
	A sequence of random variables $\{X_{t}\}_{t=0}^{\infty}$ is a \emph{Markov chain}
	if it satisfies the Markov property.
\end{definition}

\begin{definition}[Initial Distribution]
	Let $\{X_{t}\}$ be a Markov chain. 
	The distribution of $X_{0}$ is called the \emph{initial distribution}.
\end{definition}

\section{High-Order Transitions}

\begin{definition}[High-Order Transition Probabilities]
	The $n$-th order transition probability from $i$ to $j$ is defined as
	\begin{align*}
		P_{ij}^{(n)}
		\coloneqq
		\pr{X_{n+m}=j\given X_{m}=i}.
	\end{align*}
\end{definition}

\begin{theorem}[Chapman-Kolmogorov's Identity]
	For any $n,m\in\naturals$,
	the $(n+m)$-th order transition probability can be decomposed thus:
	\begin{align*}
		P_{ij}^{(n+m)} = \sum_{k\in S} P_{ik}^{(n)}P_{kj}^{(m)}.
	\end{align*}
\end{theorem}

\section{Classification of States}

\begin{definition}[Persistent/Transient]
	Define
	\begin{align*}
		f_{ij} \coloneqq
		\pr{\exists n\geq1 \text{ s.t. }X_{n}=j\given X_{0}=i}.
	\end{align*}
	A state $i\in S$ is said to be \emph{persistent} if $f_{ii}=1$.
	A state $i\in S$ is said to be \emph{transient} if $f_{ii}<1$.
\end{definition}

\begin{remark}
	The interpretation of $f_{ij}$ is ``the probability that $j$ will be visited at some point in time starting from $i$.''
\end{remark}

\begin{theorem}
	The following statements are equivalent:
	\begin{enumerate}[label=(\roman*)]
		\item State $i\in S$ is persistent.
		\item $\pr{X_{n}=i\inftyoften\given X_{0}=i}=1$.
		\item $\sum_{n\geq1} P_{ii}^{(n)}=+\infty$.
	\end{enumerate}
	The following statements are equivalent:
	\begin{enumerate}[label=(\roman*)]
		\item State $i\in S$ is transient.
		\item $\pr{X_{n}=i\inftyoften\given X_{0}=i}=0$.
		\item $\sum_{n\geq1} P_{ii}^{(n)}<\infty$.
	\end{enumerate}
\end{theorem}

\section{Classification of Markov Chains}

\begin{definition}[Irreducible]
	A Markov chain is called \emph{irreducible} if $\forall i,j\in S$ $\exists n\in\naturals$ such that
	$P_{ij}^{(n)}>0$.
\end{definition}

\begin{theorem}
	Assume that a Markov chain is irreducible,
	then either all states are transient,
	or all states are persistent.
\end{theorem}

\begin{corollary}
	If $S$ is finite and irreducible, then the Markov chain is always persistent.
\end{corollary}

\section{Stationary Distribution}

\begin{definition}[Stationary Distribution]
	Let $P_{ij}$ be transition probabilities.
	A distribution $\pi=\{\pi_{i}\}_{i\in S}$ is said to be a \emph{stationary distribution} of a Markov chain
	if $\forall i,j\in S$,
	\begin{align*}
		\sum_{i\in S} \pi_{i}P_{ij} = \pi_{j}.
	\end{align*}
\end{definition}

\begin{remark}
	If $S$ is finite,
	then we can let $\pi=(\pi_{1},...,\pi_{|S|})$ and rewrite the condition as
	$\pi P=\pi$ where $P$ is the transition matrix.
\end{remark}

\begin{definition}[Period]
	A state $i$ has period $t=\gcd\{n\geq1\given P_{ii}^{(n)}>0\}$.
\end{definition}

\begin{definition}[Aperiodic]
	A Markov chain is said to be \emph{aperiodic} if all states have period one.
\end{definition}

\begin{definition}
	Suppose that a Markov chain is irreducible and aperiodic.
	Also suppose that a stationary distribution $\{\pi_{i}\}$ exists.
	Then,
	\begin{enumerate}[label=(\roman*)]
		\item
			The chain is persistent.
		\item
			The limit of high-order probabilities converge:
			\begin{align*}
				\lim_{n\to\infty} P^{(n)}_{ij} = \pi_{j} \quad\forall i,j\in S.
			\end{align*}
		\item
			All $\pi_{i}$ are positive.
		\item
			The stationary distribution is unique.
	\end{enumerate}
\end{definition}

\begin{remark}
	In this theorem,
	the effect of the initial distribution wears off as $n\to\infty$ if a stationary distribution exists.
\end{remark}

\begin{theorem}
	If an irreducible and aperiodic Markov chain has no stationary distribution, then
	\begin{align*}
		\lim_{n\to\infty} P_{ij}^{(n)} = 0 \text{ as } n\to\infty
	\end{align*}
\end{theorem}

% TODO: existence of stationary distribution.

\section{Markov Chain Monte Carlo}

\subsection{Metropolis-Hastings Algorithm}

% TODO: implement in R

Let $\pi$ be a distribution.
We want to construct a Markov chain such that its stationary distribution is $\pi$.
That is, we want to find transition probabilities $P_{ij}$'s such that
\begin{align*}
	\sum_{i} \pi(i) P_{ij} = \pi(j) \quad\forall j.
	\tag{Global Balance Equation}
\end{align*}
Since the global balance equation requires a summation over all states $i$,
it is not easy to construct.
However, we can find $P_{ij}$'s such that
\begin{align*}
	\pi(i)P_{ij} = \pi(j)P_{ji} \quad\forall i,j.
	\tag{Detailed Balance Equation}
\end{align*}
Notice that detailed balance equation implies global balance equation since
\begin{align*}
	\sum_{i} \pi(i)P_{ij}
	= \sum_{i} \pi(j)P_{ji}
	= \pi(j) \sum_{i} P_{ji}
	= \pi(j).
\end{align*}
Since the detailed balance equation is only a requirement for each pair of states $i$ and $j$,
it is much more easy to construct the required $P_{ij}$ and $P_{ji}$.
This motivates the Metropolis-Hastings algorithm.

\begin{theorem}[Metropolis-Hastings Algorithm]\label{thm:M-H}
	Given a target distribution $\pi$,
	construct a Markov chain as follows.
	Arbitrarily fix an initial state $I_{0}$.
	\begin{enumerate}
		\item
			Generate $J\sim\{P_{ij}\}$ where $\{P_{ij}\}$ is an arbitrary distribution (transition probabilities) with the same support as $\pi$.
			Let $j$ be the generated state.
			That is,
			\begin{align*}
				\pr{J=j\given I_{n}=i}=P_{ij}.
			\end{align*}
			Distribution $\{P_{ij}\}$ is called the proposal distribution.
		\item
			Set the value of $I_{n+1}$ according to the following rule:
			\begin{gather*}
				I_{n+1} =
				\begin{cases}
					j & \text{ with probability $\alpha(i,j)$ } \\
					i & \text{ otherwise }
				\end{cases}
				\shortintertext{where}
				\alpha(i,j)
				= \min\left\{\frac{\pi(j)P_{ji}}{\pi(i)P_{ij}},1\right\}.
			\end{gather*}
	\end{enumerate}
	Then, the Markov chain $\{I_{n}\}$ has stationary distribution $\pi$.
\end{theorem}

\begin{remark}
	We can also summarize the result thus:
	The transition probabilities $\{\tilde{P}_{ij}\}$ where
	$\tilde{P}_{ij} \coloneqq \alpha(i,j)P_{ij}$
	has stationary distribution $\pi$.
\end{remark}

\begin{remark}
	In many applications, the proposal distribution is not drawn from a chain,
	but from an iid distribution.
	Thus, the ratio $(\pi(i)P_{ij})/(\pi(j)P_{ji})$ simplifies to $(\pi(i)f(i))/(\pi(j)f(j))$
	with $f$ being the proposal density.
\end{remark}

\begin{proof}(of Theorem \ref{thm:M-H})
	We only have to show that the constructed transition probabilities satisfies the detailed balance equation.
	Let $i,j$ be an arbitrary pair of states.
	We have $\tilde{P}_{ij}=\alpha(i,j)P_{ij}$.
	Consider $\pi(i)$:
	\begin{align*}
		\pi(i)\tilde{P}_{ij}
		= \pi(i)\min\left\{\frac{\pi(j)P_{ji}}{\pi(i)P_{ij}},1\right\}P_{ij}
		=
		\begin{cases}
			\pi(j)P_{ji} & \text{ if } \pi(j)P_{ji} \leq \pi(i)P_{ij} \\
			\pi(i)P_{ij} & \text{ if } \pi(j)P_{ji} \geq \pi(i)P_{ij}.
		\end{cases}
	\end{align*}
	On the other hand, consider $\pi(j)$:
	\begin{align*}
		\pi(j)\tilde{P}_{ji}
		= \pi(j)\min\left\{\frac{\pi(i)P_{ij}}{\pi(j)P_{ji}},1\right\}P_{ji}
		=
		\begin{cases}
			\pi(i)P_{ij} & \text{ if } \pi(i)P_{ij} \leq \pi(j)P_{ji} \\
			\pi(j)P_{ji} & \text{ if } \pi(i)P_{ij} \geq \pi(j)P_{ji}.
		\end{cases}
	\end{align*}
	Therefore, the detailed balance equation is satisfied.
\end{proof}

\subsection{Gibbs Sampling}

Gibbs sampling is a variation of the Metropolis-Hastings algorithm
that makes sampling high-dimensional distributions more efficient.

For illustration, let $\pi(a,b)$ be a distribution on $A\times B$ for $a\in A$ and $b\in B$.
Following the Metropolis-Hasting algorithm, we can view the pair $(a,b)$ as a single state and perform the sampling.
However, if $\pi(a\given b)$ and $\pi(b\given a)$ are easy to sample from, then we can sample $a$ and $b$ in alternation.

\begin{theorem}[Gibbs Sampling]\label{thm:gibbs}
	Given a target distribution $\pi(a,b)$,
	construct a Markov chain as follows.
	Arbitrarily fix an initial state $(a_0,b_0)$.
	In the iteration at time $t$, perform the following:
	\begin{enumerate}
		\item
			Generate $a_{t+1}$ from $\pi(a\given b_{t})$.
		\item
			Generate $b_{t+1}$ from $\pi(b\given a_{t+1})$
	\end{enumerate}
	Then, Markov Chain $\{(a_{t},b_{t})\}_{t}$ has stationary distribution $\pi(a,b)$.
\end{theorem}
\begin{proof}
	Following the proof of Theorem \ref{thm:M-H},
	note that $\pi(a\given b)$ and $\pi(b\given a)$ are proposal distributions.
	Thus, we only have to show that under these proposal distributions, 
	we always accept the proposed state, i.e., the $\alpha(i,j)$ in the proof of Theorem \ref{thm:M-H} is always $1$.

	Let $(a,b)$ be the current state and let $a'\sim\pi(a\given b)$.
	Consider the following:
	\begin{align*}
		\alpha((a,b),(a',b))
		&= \min\left\{\frac{\pi(a',b)}{\pi(a,b)}\frac{\pi(a\given b)}{\pi(a'\given b)},1\right\} \\
		&= \min\left\{\frac{\pi(a'\given b)\pi(b)}{\pi(a\given b)\pi(b)}\frac{\pi(a\given b)}{\pi(a'\given b)},1\right\}
		= 1.
	\end{align*}
	Similarly, we can also show that the proposed $b'\sim\pi(b\given a)$ is also always accepted.
\end{proof}

\begin{remark}
	Something remarkable about Gibbs sampling is that there is no ``rejection,''
	that is, we always ``accept'' the proposed state through clever choice of proposal distribution $\pi(a\given b)$ and $\pi(b\given a)$.
	In practice, we often know how to sample from $\pi(a\given b)$ and $\pi(b\given a)$ directly, but do not know the joint distribution $\pi(a,b)$.
	Thus, Gibbs sampling provides a very efficient way of sampling $\pi(a,b)$.
\end{remark}

\end{document}
